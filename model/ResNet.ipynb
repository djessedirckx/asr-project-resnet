{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ib7R-sYEQwFp"
   },
   "source": [
    "# ResNet Model specification and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0mdbxg4QwFs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import time\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tensorflow neural network imports\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, BatchNormalization, Add, AveragePooling2D, ReLU, GlobalAveragePooling2D\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "# Util imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yRdPv6aQwFt"
   },
   "source": [
    "## Loading and pre-processing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woe37z0wQwFu"
   },
   "outputs": [],
   "source": [
    "# Load data and labels (allow_pickle is required since not all sequences have the same length (\n",
    "# object type = 'object'))\n",
    "data = np.load('resnet_mfcc_features_test.npy')\n",
    "labels = np.load('resnet_labels_test.npy')\n",
    "\n",
    "# Create a dummy encoding of the labels\n",
    "encoded_labels = pd.get_dummies(labels).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qV1qC0G6QwFv"
   },
   "outputs": [],
   "source": [
    "# Splitting of the data into train and validation data (0.8 - 0.2)\n",
    "data_train, data_val, labels_train, labels_val = train_test_split(data, encoded_labels, train_size=0.67, random_state=42, stratify=encoded_labels)\n",
    "\n",
    "# Split validation data into validation and test data (0.1 - 0.1)\n",
    "data_val, data_test, labels_val, labels_test = train_test_split(data_val, labels_val, train_size=0.5, random_state=42, stratify=labels_val)\n",
    "\n",
    "# Clean up memory\n",
    "del data\n",
    "del labels\n",
    "del encoded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWUN1UpbQwFv"
   },
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxjEHgDWQwFv"
   },
   "outputs": [],
   "source": [
    "class ResNetModel():\n",
    "    \n",
    "    def __init__(self, input_shape, n_classes=35, res_depth=6, conv_filters=45, use_dil_scheme=True):\n",
    "        self.input_shape = input_shape\n",
    "        self.n_classes = n_classes\n",
    "        self.res_depth = res_depth\n",
    "        self.conv_filters = conv_filters\n",
    "        self.use_dil_scheme = use_dil_scheme\n",
    "        \n",
    "    def compute_dilation(self, index, use_scheme):\n",
    "        if use_scheme:\n",
    "            dilation = int(2**(index//3))\n",
    "            index += 1\n",
    "            return dilation, index\n",
    "        else:\n",
    "            return 1, 1\n",
    "        \n",
    "    def residual_block(self, input_layer, index, use_scheme):\n",
    "        \n",
    "        # Apply convolutions on output of previous layer\n",
    "        \n",
    "        dilation, index = self.compute_dilation(index, use_scheme)\n",
    "        x = Conv2D(filters = self.conv_filters, kernel_size = 3, activation = 'relu', padding = 'same', dilation_rate=dilation)(input_layer)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        dilation, index = self.compute_dilation(index, use_scheme)\n",
    "        x = Conv2D(filters = self.conv_filters, kernel_size = 3, activation = 'relu', padding = 'same', dilation_rate=dilation)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        # Add output of convolutional operation to the input\n",
    "        addition = Add()([x, input_layer])\n",
    "\n",
    "        return addition, index\n",
    "    \n",
    "    def build_model(self):\n",
    "        \n",
    "        # Define the input, consisting of a single convolutional layer\n",
    "        input_layer = Input(self.input_shape)\n",
    "        x = Conv2D(filters = self.conv_filters, kernel_size = 3, activation = 'relu', padding = 'same', use_bias=False)(input_layer)\n",
    "        \n",
    "        # Create the desired number of residual blocks\n",
    "        conv_index = 0\n",
    "        for i in range(self.res_depth):\n",
    "            x, conv_index = self.residual_block(x, conv_index, self.use_dil_scheme)\n",
    "        \n",
    "        # Add non-residual conv and bn layer\n",
    "        dilation, index = self.compute_dilation(conv_index, self.use_dil_scheme)\n",
    "        x = Conv2D(filters = self.conv_filters, kernel_size = 3, activation = 'relu', padding = 'same', dilation_rate=dilation)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        # Final pooling and output layer\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        output = Dense(self.n_classes, activation='softmax')(x)\n",
    "        \n",
    "        return Model(inputs=[input_layer], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u94UQNRfQwFx",
    "outputId": "11fec2a8-ce37-42e3-bc11-ce43a9fd36ba",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1, 138, 40)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 1, 138, 45)   16200       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 1, 138, 45)   18270       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1, 138, 45)   180         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 1, 138, 45)   18270       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1, 138, 45)   180         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 1, 138, 45)   0           batch_normalization_1[0][0]      \n",
      "                                                                 conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 1, 138, 45)   18270       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1, 138, 45)   180         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 1, 138, 45)   18270       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 138, 45)   180         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1, 138, 45)   0           batch_normalization_3[0][0]      \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 1, 138, 45)   18270       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1, 138, 45)   180         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 1, 138, 45)   18270       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 1, 138, 45)   180         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 1, 138, 45)   0           batch_normalization_5[0][0]      \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 1, 138, 45)   18270       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 1, 138, 45)   180         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 1, 138, 45)   18270       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 1, 138, 45)   180         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1, 138, 45)   0           batch_normalization_7[0][0]      \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 1, 138, 45)   18270       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 1, 138, 45)   180         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 1, 138, 45)   18270       batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 1, 138, 45)   180         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 1, 138, 45)   0           batch_normalization_9[0][0]      \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 1, 138, 45)   18270       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 1, 138, 45)   180         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 1, 138, 45)   18270       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 1, 138, 45)   180         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 1, 138, 45)   0           batch_normalization_11[0][0]     \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 1, 138, 45)   18270       add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 1, 138, 45)   180         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 45)           0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 35)           1610        global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 257,660\n",
      "Trainable params: 256,490\n",
      "Non-trainable params: 1,170\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ResNetModel([1, 138,40]).build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFz4VgTkQwFy"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqXDbYSqQwF0"
   },
   "outputs": [],
   "source": [
    "def create_data_split(data, labels, train_size):\n",
    "    '''\n",
    "    This function can be used to use smaller portions of training data to investigate\n",
    "    how this influences performance.\n",
    "    '''\n",
    "    \n",
    "    # Compute relative percentage of training data to be used\n",
    "    train_size = round(train_size / 0.8, 3)\n",
    "\n",
    "    data_train, _, labels_train, _ = train_test_split(data, labels, train_size=train_size, random_state=42, stratify=labels)\n",
    "    \n",
    "    return data_train, labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHX0j_FuCtZY"
   },
   "outputs": [],
   "source": [
    "# Decrease the amount of training data to the desired percentage\n",
    "\n",
    "# data_train, labels_train = create_data_split(data_train, labels_train, train_size=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gzg-2CVGQwF0",
    "outputId": "0e16a171-4ff4-41b9-d613-0762c4b2e7c4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model on training data\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit(data_train, labels_train, validation_data=(data_val,labels_val), epochs=EPOCHS,batch_size=BATCH_SIZE)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time elapsed during training: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3qGwHTRQwF1"
   },
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "test_predictions = model.predict(data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWoPv7wSQwF1"
   },
   "source": [
    "## Store predictions, training output and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t53qaFe9QwF1"
   },
   "outputs": [],
   "source": [
    "# Store predictions and training history to harddrive\n",
    "\n",
    "file_name = \"(resnet-wavenet-base)\"\n",
    "with open(f'{file_name}_predictions.npy', 'wb') as pred_file:\n",
    "    np.save(pred_file, test_predictions)\n",
    "\n",
    "with open(f'{file_name}_test_labels.npy', 'wb') as lab_file:\n",
    "    np.save(lab_file, labels_val)\n",
    "    \n",
    "with open(f'{file_name}_training_history.npy', 'wb') as hist_fil:\n",
    "    np.save(hist_fil, history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1634J8zqmAb6",
    "outputId": "8d11ad47-74ab-45cb-c087-81412c827fa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions are equal: True\n",
      "Test labels are equal: True\n",
      "History export is equal: True\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "print(f\"Test predictions are equal: {np.array_equal(np.load(f'{file_name}_predictions.npy'), test_predictions)}\")\n",
    "print(f\"Test labels are equal: {np.array_equal(np.load(f'{file_name}_test_labels.npy'), labels_val)}\")\n",
    "print(f\"History export is equal: {np.array_equal(np.load(f'{file_name}_training_history.npy', allow_pickle=True), history.history)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Drn-7bzlk81v",
    "outputId": "d561e6f3-b587-4da1-d9c7-86185c25c144"
   },
   "outputs": [],
   "source": [
    "# Export the model\n",
    "model.save(f\"{file_name}_model.hdf5\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ResNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
